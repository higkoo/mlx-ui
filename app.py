import re
import time

import mlx.core as mx
import streamlit as st
from mlx_lm import load
from mlx_lm.generate import generate_step
from mlx_lm.sample_utils import make_sampler
import argparse

title = "æ™ºèƒ½åŠ©æ‰‹"
ver = "0.8.1"
debug = True


def generate(the_prompt, the_model):
    tokens = []
    skip = 0
    count = 0
    
    # ç®€åŒ–çš„é‡å¤æ£€æµ‹ï¼šåªæ£€æµ‹è¿ç»­çš„å®Œå…¨é‡å¤
    last_complete_response = ""
    repeat_count = 0
    max_repeats = 2
    
    # ç¼–ç æç¤º
    input_ids = mx.array(tokenizer.encode(the_prompt))
    
    # åˆ›å»ºé‡‡æ ·å™¨ï¼Œè®¾ç½®ç”¨æˆ·è¦æ±‚çš„å‚æ•°
    # å‚æ•°è¯´æ˜ï¼š
    # - temp: æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆçš„éšæœºæ€§ï¼Œ0.6 æ˜¯ä¸€ä¸ªå¹³è¡¡å€¼
    # - top_p: æ ¸é‡‡æ ·å‚æ•°ï¼Œæ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§ï¼Œ0.95 ä¿ç•™ 95% çš„æ¦‚ç‡è´¨é‡
    # - top_k: ä¿ç•™æ¦‚ç‡æœ€é«˜çš„ k ä¸ª tokenï¼Œ20 æ˜¯ä¸€ä¸ªé€‚ä¸­å€¼
    # - min_p: æœ€å°æ¦‚ç‡é˜ˆå€¼ï¼Œ0 è¡¨ç¤ºä¸ä½¿ç”¨æ­¤åŠŸèƒ½
    # - min_tokens_to_keep: æœ€å°ä¿ç•™çš„ token æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 1
    sampler = make_sampler(
        temp=0.6,
        top_p=0.95,
        min_p=0,
        min_tokens_to_keep=1,
        top_k=20,
        xtc_probability=0.0,
        xtc_threshold=0.0,
        xtc_special_tokens=[]
    )
    
    # å¼€å§‹ç”Ÿæˆï¼Œä¼ é€’é‡‡æ ·å™¨
    gen = generate_step(
        input_ids, 
        the_model, 
        sampler=sampler,
        max_tokens=context_length  # è®¾ç½®æœ€å¤§ç”Ÿæˆçš„ token æ•°
    )
    
    # å¾ªç¯ç”Ÿæˆï¼Œç›´åˆ°ç”Ÿæˆå™¨åœæ­¢æˆ–è¾¾åˆ°æœ€å¤§ token æ•°
    for token, prob in gen:
        tokens.append(token)
        text = tokenizer.decode(tokens)
        current_chunk = text[skip:]
        
        # è¾“å‡ºå½“å‰ç”Ÿæˆçš„æ–‡æœ¬
        yield current_chunk
        
        # æ›´æ–°åç§»é‡å’Œè®¡æ•°
        skip = len(text)
        count += 1
        
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§ token æ•°
        if count >= context_length:
            break


def show_chat(the_prompt, previous=""):
    if debug:
        print(the_prompt)
        print("-" * 80)

    with ((st.chat_message("assistant"))):
        message_placeholder = st.empty()
        response = previous

        # ç”Ÿæˆå¹¶æ˜¾ç¤ºå†…å®¹
        for chunk in generate(the_prompt, model):
            response = response + chunk

            if not previous:
                # begin neural-beagle-14 fixes
                response = re.sub(r"^/\*+/", "", response)
                response = re.sub(r"^:+", "", response)
                # end neural-beagle-14 fixes

            # ç§»é™¤æ‰€æœ‰ä¸éœ€è¦çš„æ ‡ç­¾
            response = re.sub(r"<think>", "", response)
            response = re.sub(r"</think>", "", response)
            response = re.sub(r"<\|im_start\|>", "", response)
            response = re.sub(r"<\|im_end\|>", "", response)
            response = re.sub(r"<\|endoftext\|>", "", response)
            response = re.sub(r"<s>", "", response)
            response = re.sub(r"</s>", "", response)
            
            # ç§»é™¤é‡å¤çš„ "Human:" æ ‡è®°
            response = re.sub(r"Human:", "", response)
            
            # ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
            response = re.sub(r"\n{3,}", "\n\n", response)
            
            # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
            response = response.replace('ï¿½', '')
            
            # å®æ—¶æ˜¾ç¤ºç”Ÿæˆçš„å†…å®¹
            message_placeholder.markdown(response + "â–Œ")

        # ç”Ÿæˆå®Œæˆåï¼Œæ¸…ç†æœ€ç»ˆå†…å®¹
        # 1. ç§»é™¤å¤šä½™çš„ç©ºè¡Œ
        final_response = re.sub(r"\n{3,}", "\n\n", response)
        
        # 2. ç¡®ä¿å†…å®¹æ ¼å¼æ­£ç¡®
        final_response = final_response.strip()
        
        # 3. æ˜¾ç¤ºæœ€ç»ˆæ¸…ç†åçš„å†…å®¹
        message_placeholder.markdown(final_response)

    # å°†æœ€ç»ˆå†…å®¹æ·»åŠ åˆ°ä¼šè¯çŠ¶æ€
    st.session_state.messages.append({"role": "assistant", "content": final_response})
    
    # ç§»é™¤è‡ªåŠ¨ç»§ç»­ç”Ÿæˆé€»è¾‘ï¼Œæ”¹ä¸ºé€šè¿‡è°ƒæ•´ç”Ÿæˆå‚æ•°æ¥é¿å…ä¸­é€”åœæ­¢
    # è¿™æ ·å¯ä»¥é¿å…é‡å¤å†…å®¹é—®é¢˜


def remove_last_occurrence(array, criteria_fn):
    for i in reversed(range(len(array))):
        if criteria_fn(array[i]):
            del array[i]
            break


def build_memory():
    # é™åˆ¶å¯¹è¯å†å²çš„é•¿åº¦ï¼Œåªä¿ç•™æœ€è¿‘çš„ 5 æ¡æ¶ˆæ¯
    max_history_length = 5
    if len(st.session_state.messages) > 2:
        # ä¿ç•™æœ€è¿‘çš„ max_history_length æ¡æ¶ˆæ¯
        return st.session_state.messages[max(1, len(st.session_state.messages) - max_history_length):-1]
    return []


def queue_chat(the_prompt, continuation=""):
    # workaround because the chat boxes are not really replaced until a rerun
    st.session_state["prompt"] = the_prompt
    st.session_state["continuation"] = continuation
    st.rerun()


# tx @cocktailpeanut
parser = argparse.ArgumentParser(description="mlx-ui")
parser.add_argument("--models", type=str, help="the txt file that contains the models list", default="models.txt")
args = parser.parse_args()
models_file = args.models

assistant_greeting = "æˆ‘èƒ½ä¸ºæ‚¨æä¾›ä»€ä¹ˆå¸®åŠ©ï¼Ÿ"

with open(models_file, 'r') as file:
    model_refs = [line.strip() for line in file.readlines() if not line.startswith('#')]

model_refs = {k.strip(): v.strip() for k, v in [line.split("|") for line in model_refs]}

st.set_page_config(
    page_title=title,
    page_icon="ğŸ¤–",
    layout="wide",
    initial_sidebar_state="expanded",
)
st.title(title)

st.markdown("""
<style>
.stDeployButton{display:none}
/* ä¿®æ”¹è¿è¡ŒçŠ¶æ€æ–‡æœ¬ä¸ºä¸­æ–‡ */
[data-testid='stStatusWidget'] {
    position: relative;
}
[data-testid='stStatusWidget'] span,
[data-testid='stStatusWidget'] div {
    display: none !important;
}
[data-testid='stStatusWidget']::before {
    content: 'è¿è¡Œä¸­...';
    display: inline-block;
    margin-right: 10px;
}
[data-testid='stStatusWidget'] button {
    font-size: 14px !important;
}
[data-testid='stStatusWidget'] button span {
    display: none !important;
}
[data-testid='stStatusWidget'] button::after {
    content: 'åœæ­¢';
    display: inline-block;
}
</style>
""", unsafe_allow_html=True)


import os

@st.cache_resource(show_spinner=True)
def load_model_and_cache(ref):
    # å±•å¼€æœ¬åœ°è·¯å¾„ä¸­çš„ ~ ç¬¦å·
    if os.path.exists(os.path.expanduser(ref)):
        ref = os.path.expanduser(ref)
    return load(ref, {"trust_remote_code": True})


model = None

model_ref = st.sidebar.selectbox("æ¨¡å‹", model_refs.keys(), format_func=lambda value: model_refs[value],
                                 help="æŸ¥çœ‹ https://modelscope.cn è·å–æ›´å¤šæ¨¡å‹ã€‚å°†æ‚¨å–œæ¬¢çš„æ¨¡å‹æ·»åŠ åˆ° models.txt æ–‡ä»¶ä¸­ã€‚")

if model_ref.strip() != "-":
    model, tokenizer = load_model_and_cache(model_ref)

    chat_template = tokenizer.chat_template or (
        "{% for message in messages %}"
        "{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}"
        "{% endfor %}"
        "{% if add_generation_prompt %}"
        "{{ '<|im_start|>assistant\n' }}"
        "{% endif %}"
    )
    supports_system_role = "system role not supported" not in chat_template.lower()

    system_prompt = st.sidebar.text_area("ç³»ç»Ÿæç¤º", "ä½ æ˜¯ä¸€ä½æ™ºæ…§çš„AIåŠ©æ‰‹ï¼ŒåŸºäºå¤§é‡äººç±»çŸ¥è¯†è®­ç»ƒè€Œæˆã€‚åœ¨å›ç­”é—®é¢˜æ—¶ï¼Œè¯·ç›´æ¥ç»™å‡ºæœ€ç»ˆç­”æ¡ˆï¼Œä¸éœ€è¦ä½¿ç”¨ä»»ä½•ç‰¹æ®Šæ ‡ç­¾æˆ–æ ‡è®°ã€‚å›ç­”è¦ç®€æ´æ˜äº†ï¼Œç›´æ¥é’ˆå¯¹é—®é¢˜ã€‚é‡è¦ï¼šä¸è¦é‡å¤ä¹‹å‰çš„å†…å®¹ï¼Œä¸è¦é‡å¤ç›¸åŒçš„æ®µè½æˆ–å¥å­ã€‚",
                                         disabled=not supports_system_role)

    context_length = st.sidebar.number_input('ä¸Šä¸‹æ–‡é•¿åº¦', value=2048, min_value=99, step=100, max_value=32000,
                                             help="å¤§è‡´æ‰“å°çš„æœ€å¤§å•è¯æ•°ã€‚")

    st.sidebar.markdown("---")
    actions = st.sidebar.columns(2)

    # give a bit of time for sidebar widgets to render
    time.sleep(0.05)

    if "messages" not in st.session_state:
        st.session_state["messages"] = [{"role": "assistant", "content": assistant_greeting}]

    stop_words = ["<|im_start|>", "<|im_end|>", "<s>", "</s>"]

    if actions[0].button("ğŸ˜¶â€ğŸŒ«ï¸ æ¸…ç©º", use_container_width=True,
                         help="æ¸…ç©ºä¹‹å‰çš„å¯¹è¯ã€‚"):
        st.session_state.messages = [{"role": "assistant", "content": assistant_greeting}]
        if "prompt" in st.session_state and st.session_state["prompt"]:
            st.session_state["prompt"] = None
            st.session_state["continuation"] = None
        st.rerun()

    if actions[1].button("ğŸ”‚ ç»§ç»­", use_container_width=True,
                         help="ç»§ç»­ç”Ÿæˆã€‚"):

        user_prompts = [msg["content"] for msg in st.session_state.messages if msg["role"] == "user"]

        if user_prompts:

            last_user_prompt = user_prompts[-1]

            assistant_responses = [msg["content"] for msg in st.session_state.messages
                                   if msg["role"] == "assistant" and msg["content"] != assistant_greeting]
            last_assistant_response = assistant_responses[-1] if assistant_responses else ""

            # remove last line completely, so it is regenerated correctly (in case it stopped mid-word or mid-number)
            last_assistant_response_lines = last_assistant_response.split('\n')
            if len(last_assistant_response_lines) > 1:
                last_assistant_response_lines.pop()
                last_assistant_response = "\n".join(last_assistant_response_lines)

            messages = [
                {"role": "user", "content": last_user_prompt},
                {"role": "assistant", "content": last_assistant_response},
            ]
            if supports_system_role:
                messages.insert(0, {"role": "system", "content": system_prompt})

            full_prompt = tokenizer.apply_chat_template(messages,
                                                        tokenize=False,
                                                        add_generation_prompt=False,
                                                        chat_template=chat_template)
            full_prompt = full_prompt.rstrip("\n")

            # remove last assistant response from state, as it will be replaced with a continued one
            remove_last_occurrence(st.session_state.messages,
                                   lambda msg: msg["role"] == "assistant" and msg["content"] != assistant_greeting)

            queue_chat(full_prompt, last_assistant_response)

    if prompt := st.chat_input("èŠç‚¹ä»€ä¹ˆ..."):
        st.session_state.messages.append({"role": "user", "content": prompt})

        messages = []
        if supports_system_role:
            messages += [{"role": "system", "content": system_prompt}]
        messages += build_memory()
        messages += [{"role": "user", "content": prompt}]

        full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True,
                                                    chat_template=chat_template)
        full_prompt = full_prompt.rstrip("\n")

        queue_chat(full_prompt)

    for msg in st.session_state.messages:
        st.chat_message(msg["role"]).write(msg["content"])

    # give a bit of time for messages to render
    time.sleep(0.05)

    if "prompt" in st.session_state and st.session_state["prompt"]:
        show_chat(st.session_state["prompt"], st.session_state["continuation"])
        st.session_state["prompt"] = None
        st.session_state["continuation"] = None

st.sidebar.markdown("---")
st.sidebar.markdown(f"ç‰ˆæœ¬ v{ver} / Streamlit {st.__version__}")
